import hashlib
import asyncio
from functools import wraps
from typing import List, Union
from diskcache import Cache
from openai import AsyncOpenAI
from rich.progress import (
    Progress,
    SpinnerColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    MofNCompleteColumn,
)
from rich.table import Table
from rich.console import Console 
import json
import tiktoken
import os
import datetime
from utils.utils_control_logger import control_logger as logger

from utils.utils_gpt_logger import gpt_logger

# ----------------------------------------------------------------------
# å…¨å±€å˜é‡å’Œåˆå§‹åŒ–
# ----------------------------------------------------------------------
# åˆ›å»ºç¼“å­˜å®ä¾‹
cache = Cache("./openai_cache")
# å…¨å±€ token è®¡æ•°å™¨
_total_tokens = 0
_input_tokens = 0     # è¾“å…¥ token æ•°é‡
_output_tokens = 0    # è¾“å‡º token æ•°é‡
waiting_count = 0     # å…¨å±€ç­‰å¾…è®¡æ•°å™¨

# é€Ÿç‡é™åˆ¶ç›¸å…³çš„å…¨å±€å˜é‡ï¼ˆé’ˆå¯¹ä¸åŒäº‹ä»¶å¾ªç¯çš„é”å’Œä¸Šæ¬¡è¯·æ±‚æ—¶é—´ï¼‰
_rate_limit_locks = {}
_last_request_times = {}

# å“åº”æ—¶é—´ç»Ÿè®¡ï¼Œç”¨äºè®°å½•æ¯ä¸ªéƒ¨ç½²çš„ç»Ÿè®¡æ•°æ®
_deployment_stats = {}

# é€€é¿ç­–ç•¥ç›¸å…³å˜é‡
_base_delay = 1   # åŸºç¡€å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
_max_delay = 60   # æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

# ----------------------------------------------------------------------
# éƒ¨ç½²ç»Ÿè®¡ç±»
# ----------------------------------------------------------------------
class DeploymentStats:
    def __init__(self):
        self.total_time = 0       # æ€»å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
        self.request_count = 0    # è¯·æ±‚æ€»æ¬¡æ•°
        self.total_tokens = 0     # æ€» tokens æ•°

    @property
    def average_tokens_per_second(self):
        return self.total_tokens / self.total_time if self.total_time > 0 else 0

    def add_response_time(self, time,total_tokens):
        self.total_time += time
        self.request_count += 1
        self.total_tokens += total_tokens

# ----------------------------------------------------------------------
# é€Ÿç‡æ§åˆ¶å‡½æ•°ï¼ˆåŠ¨æ€å»¶è¿Ÿï¼‰
# ----------------------------------------------------------------------
async def rate_limit():
    """
    é€Ÿç‡æ§åˆ¶å‡½æ•°ï¼šæ¯æ¬¡è¯·æ±‚å‰æ£€æŸ¥ä¸Šæ¬¡è¯·æ±‚æ—¶é—´ï¼Œè‹¥ä¸è¶³ _base_delay åˆ™ç­‰å¾…ã€‚
    """
    delay = _base_delay
    loop = asyncio.get_running_loop()
    if loop not in _rate_limit_locks:
        _rate_limit_locks[loop] = asyncio.Lock()
        _last_request_times[loop] = loop.time()
    
    async with _rate_limit_locks[loop]:
        now = loop.time()
        last_request_time = _last_request_times[loop]
        elapsed = now - last_request_time
        wait_time = delay - elapsed
        if wait_time > 0:
            await asyncio.sleep(wait_time)
        _last_request_times[loop] = loop.time()

# ----------------------------------------------------------------------
# è£…é¥°å™¨ï¼šç¼“å­˜ OpenAI API å“åº”
# ----------------------------------------------------------------------
def async_cache_openai_response(func):
    """OpenAI API å“åº”çš„ç¼“å­˜è£…é¥°å™¨"""
    @wraps(func)
    async def wrapper(prompt: Union[str, List[str]], config):
        if isinstance(prompt, str):
            prompts = [prompt]
        else:
            prompts = prompt

        results = []
        for single_prompt in prompts:
            cache_key = hashlib.md5(
                f"{single_prompt}_{config['current_model']['type']}".encode()
            ).hexdigest()
            
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                gpt_logger.trace(
                    f"\n{'='*100}\n"
                    f"ğŸ¯ å‘½ä¸­ç¼“å­˜\n"
                    f"{'='*100}\n"
                    f"æç¤ºè¯: {single_prompt}\n"
                    f"ç¼“å­˜ç»“æœ:\n{cached_result}\n"
                    f"{'='*100}\n"
                )
                results.append(cached_result)
                continue
                
            result = await func(single_prompt, config)
            cache.set(cache_key, result)
            gpt_logger.trace(
                f"\n{'='*100}\n"
                f"ğŸ’« ç¼“å­˜æ–°ç»“æœ\n"
                f"{'='*100}\n"
                f"æç¤ºè¯: {single_prompt[:10]+'...' if len(single_prompt) > 10 else single_prompt}\n" \
                f"ç¼“å­˜å†…å®¹:\n{result[:10]+'...' if len(result) > 10 else result}\n" \
                f"{'='*100}\n"
            )
            results.append(result)
            
        return results[0] if isinstance(prompt, str) else results
    return wrapper

# ----------------------------------------------------------------------
# æ¨¡å‹é€‰æ‹©ä¸è½®è¯¢
# ----------------------------------------------------------------------
_current_model_index = 0
def get_next_model(config):
    """
    æ ¹æ®å½“å‰æ¨¡å‹ç±»å‹ï¼Œåœ¨é…ç½®ä¸­é€‰å–ä¸‹ä¸€ä¸ªå¯ç”¨æ¨¡å‹
    """
    global _current_model_index
    current_type = config['current_model']['type']
    same_type_models = [
        model for model in sorted(config['gpt_config']['models'], key=lambda x: x['priority'])
        if model['type'] == current_type
    ]
    
    if not same_type_models:
        raise Exception(f"æ²¡æœ‰æ‰¾åˆ°ç±»å‹ä¸º {current_type} çš„å¯ç”¨æ¨¡å‹ã€‚")
    
    _current_model_index = (_current_model_index + 1) % len(same_type_models)
    return same_type_models[_current_model_index]

# ----------------------------------------------------------------------
# å¼‚æ­¥è°ƒç”¨ OpenAI APIï¼ˆæ”¯æŒè¶…æ—¶é‡è¯•å’Œé€€é¿ç­–ç•¥ï¼‰
# ----------------------------------------------------------------------
@async_cache_openai_response
async def async_call_openai_api(prompt: str, config, max_retries: int = 15, initial_timeout: float = 120):
    """
    å¼‚æ­¥è°ƒç”¨ OpenAI API è¿›è¡Œæ–‡æœ¬å¤„ç†ï¼Œæ”¯æŒå¤šç«¯ç‚¹è½®è¯¢å’Œé€’å¢è¶…æ—¶é‡è¯•
    """
    global _total_tokens, _input_tokens, _output_tokens, waiting_count
    
    current_delay = _base_delay
    
    for retry_count in range(max_retries):
        await rate_limit()
        
        current_timeout = min(initial_timeout * (1.5 ** retry_count), 240)
        current_model = get_next_model(config)
        config['current_model'] = current_model
        deployment = current_model["deployment"]
        
        if deployment not in _deployment_stats:
            _deployment_stats[deployment] = DeploymentStats()
        
        client = AsyncOpenAI(
            api_key=current_model["api_key"], 
            base_url=current_model["api_base"]
        )

        waiting_count += 1
        start_time = asyncio.get_event_loop().time()
        try:
            # è®°å½•å‘å‡ºè¯·æ±‚å†…å®¹
            gpt_logger.trace(
                f"\n{'='*100}\n"
                f"ğŸ”„ OpenAI APIè°ƒç”¨\n"
                f"{'='*100}\n"
                f"æç¤ºè¯: {prompt}\n"
                f"æ¨¡å‹ç±»å‹: {current_model['type']}\n"
                f"éƒ¨ç½²: {deployment}\n"
                f"è¯·æ±‚è¶…æ—¶: {current_timeout:.2f}ç§’\n"
                f"{'='*100}\n"
            )

            completion = await asyncio.wait_for(
                client.chat.completions.create(
                    model=current_model["model"],
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯åœŸæœ¨å·¥ç¨‹é¢†åŸŸä¸“å®¶"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0
                ),
                timeout=current_timeout
            )
            
            # æˆåŠŸåé‡ç½®å»¶è¿Ÿ
            current_delay = _base_delay
            
            end_time = asyncio.get_event_loop().time()
            response_time = end_time - start_time
            _deployment_stats[deployment].add_response_time(response_time, completion.usage.total_tokens)
            
            response = completion.choices[0].message.content.strip()
            _total_tokens += completion.usage.total_tokens
            _input_tokens += completion.usage.prompt_tokens
            _output_tokens += completion.usage.completion_tokens
            
            gpt_logger.trace(
                f"\n{'='*100}\n"
                f"ğŸ“ OpenAI APIè°ƒç”¨è¯¦æƒ…\n"
                f"{'='*100}\n"
                f"æç¤ºè¯: {prompt}\n"
                f"æ¨¡å‹ç±»å‹: {current_model['type']}\n"
                f"éƒ¨ç½²: {deployment}\n"
                f"å“åº”æ—¶é—´: {response_time:.2f}ç§’\n"
                f"å¹³å‡tokenè¾“å‡ºé€Ÿåº¦: {_deployment_stats[deployment].average_tokens_per_second:.2f} tokens/ç§’\n"
                f"å“åº”å†…å®¹:\n{response}\n"
                f"æœ¬æ¬¡ä½¿ç”¨tokens: {completion.usage.total_tokens}\n"
                f"æ€»è®¡ä½¿ç”¨tokens: {_total_tokens}\n"
                f"è¾“å…¥tokens: {_input_tokens}, è¾“å‡ºtokens: {_output_tokens}\n"
                f"{'='*100}\n"
            )
            return response
            
        except (asyncio.TimeoutError, json.decoder.JSONDecodeError) as e:
            error_type = "JSONè§£ç é”™è¯¯" if isinstance(e, json.decoder.JSONDecodeError) else "è¶…æ—¶"
            gpt_logger.trace(
                f"\n{'='*100}\n"
                f"âš ï¸ OpenAI APIè°ƒç”¨{error_type}\n"
                f"éƒ¨ç½²: {deployment}\n"
                f"é‡è¯•æ¬¡æ•°: {retry_count + 1}/{max_retries}\n"
                f"é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"{'='*100}\n"
            )
            if retry_count == max_retries - 1:
                raise Exception(f"OpenAI APIåœ¨{max_retries}æ¬¡å°è¯•åä»ç„¶å¤±è´¥: {str(e)}")
            continue
            
        except Exception as e:
            error_str = str(e).lower()
            if ("429" in str(e) or 
                "rate limit" in error_str or 
                "504" in str(e) or 
                "500" in str(e) or
                # "502" in str(e) or
                "503" in str(e) or
                "gateway timeout" in error_str):
                
                current_delay = min(current_delay * 2, _max_delay)
                gpt_logger.trace(
                    f"\n{'='*100}\n"
                    f"âš ï¸ é‡åˆ°é™åˆ¶æˆ–è¶…æ—¶\n"
                    f"éƒ¨ç½²: {deployment}\n"
                    f"é”™è¯¯ç±»å‹: {'é€Ÿç‡é™åˆ¶' if '429' in str(e) else 'æœåŠ¡å™¨é”™è¯¯' if '500' in str(e) else 'Gatewayè¶…æ—¶'}\n"
                    f"ç­‰å¾…æ—¶é—´: {current_delay}ç§’\n"
                    f"é‡è¯•æ¬¡æ•°: {retry_count + 1}/{max_retries}\n"
                    f"prompt: {prompt}\n"
                    f"{'='*100}\n"
                )
                if retry_count > 10:
                    gpt_logger.warning(
                        f"âš ï¸ é‡åˆ°é™åˆ¶æˆ–è¶…æ—¶,é‡è¯•è¶…è¿‡10æ¬¡\n"
                        f"éƒ¨ç½²: {deployment}\n"
                        f"ç­‰å¾…æ—¶é—´: {current_delay}ç§’\n"
                        f"{'='*100}\n"
                    )
                await asyncio.sleep(current_delay)
                if retry_count < max_retries - 1:
                    continue

            gpt_logger.error(
                f"\n{'='*100}\n"
                f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥\n"
                f"éƒ¨ç½²: {deployment}\n"
                f"é”™è¯¯ä¿¡æ¯: {str(e)}\n"
                f"{'='*100}\n"
            )
            raise
            
        finally:
            waiting_count -= 1
            await client.close()

# ----------------------------------------------------------------------
# æ‰¹é‡å¤„ç†å¤šä¸ªæç¤ºè¯ï¼ˆä½¿ç”¨ä»»åŠ¡æ± +å®æ—¶è¿›åº¦æ¡ï¼‰
# ----------------------------------------------------------------------
def batch_get_cache(prompts: List[str], config) -> dict:
    """
    æ‰¹é‡è·å–ç¼“å­˜ç»“æœ
    
    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        config: é…ç½®ä¿¡æ¯
    
    Returns:
        dict: {cache_key: cached_result} æ ¼å¼çš„å­—å…¸
    """
    # æ‰¹é‡ç”Ÿæˆæ‰€æœ‰cache keys
    cache_keys = [
        hashlib.md5(
            f"{prompt}_{config['current_model']['type']}".encode()
        ).hexdigest()
        for prompt in prompts
    ]
    
    # ä»ç£ç›˜ç¼“å­˜è¯»å–åˆ°å†…å­˜
    cache_results = {}
    for key in cache_keys:
        result = cache.get(key)
        if result is not None:
            cache_results[key] = result
            
    return cache_results

async def batch_process_prompts(prompts: List[str], config, batch_size: int = 100, enforce_model_type: str = "default"):
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªæç¤ºè¯ï¼Œç»´æŒå›ºå®šæ•°é‡çš„å¹¶å‘ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨ Rich æ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡
    """
    # batch_size = 5 # æ‰¹å¤„ç†å¼ºåˆ¶é”å®šä¸º5
    if enforce_model_type != "default":
        # è®°å½•å½“å‰æ¨¡å‹ç±»å‹
        current_model_type_res = config['current_model']['type']
        config['current_model']['type'] = enforce_model_type

    results = [None] * len(prompts)
    tasks = set()
    next_prompt_index = 0
    global waiting_count
    
    # é¦–å…ˆæ‰¹é‡è·å–ç¼“å­˜ç»“æœ
    prompt_to_key = {
        prompt: hashlib.md5(
            f"{prompt}_{config['current_model']['type']}".encode()
        ).hexdigest()
        for prompt in prompts
    }
    key_to_prompt = {v: k for k, v in prompt_to_key.items()}
    
    # æ‰¹é‡è·å–ç¼“å­˜ç»“æœ
    cache_results = batch_get_cache(prompts, config)
    
    # å¤„ç†ç¼“å­˜å‘½ä¸­çš„ç»“æœ
    prompts_to_process = []
    for i, prompt in enumerate(prompts):
        cache_key = prompt_to_key[prompt]
        if cache_key in cache_results:
            results[i] = cache_results[cache_key]
            gpt_logger.trace(
                f"\n{'='*100}\n"
                f"ğŸ¯ å‘½ä¸­ç¼“å­˜\n"
                f"{'='*100}\n"
                f"æç¤ºè¯: {prompt}\n"
                f"ç¼“å­˜ç»“æœ:\n{cache_results[cache_key]}\n"
                f"{'='*100}\n"
            )
        else:
            prompts_to_process.append((i, prompt))

    def get_cost_description():
        input_price = 2.5
        output_price = 10

        """ç”Ÿæˆæˆæœ¬å’Œé¢„ä¼°æˆæœ¬æè¿°"""
        input_cost = (_input_tokens / 1_000_000) * input_price
        output_cost = (_output_tokens / 1_000_000) * output_price
        current_cost = input_cost + output_cost
        
        # ä½¿ç”¨ tiktoken è®¡ç®—é¢„ä¼° token
        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        
        # è®¡ç®—æ‰€æœ‰promptsçš„é¢„ä¼°tokenæ•°
        total_estimated_tokens = sum(len(encoding.encode(p)) for p in prompts)
        # å‡è®¾è¾“å‡ºtokenç­‰äºè¾“å…¥token
        total_cost = (total_estimated_tokens * 1) * (input_price / 1_000_000)  # è¾“å…¥æˆæœ¬
        total_cost += (total_estimated_tokens * 1) * (output_price / 1_000_000)*0.1  # è¾“å‡ºæˆæœ¬
        
        return (f"\nå½“å‰æˆæœ¬: Â¥{current_cost:.2f} å…ƒ"
                f" | é¢„ä¼°æ€»æˆæœ¬: Â¥{total_cost:.2f} å…ƒ")
    
    def get_stats_description():
        """ç”ŸæˆåŒ…å«å„éƒ¨ç½²å¹³å‡tokenè¾“å‡ºé€Ÿåº¦çš„æè¿°"""
        stats = []
        for deployment, stat in _deployment_stats.items():
            if stat.request_count > 0:
                stats.append(f"{deployment}: {stat.average_tokens_per_second:.2f} tokens/s")
        cost_desc = get_cost_description()
        return "\n".join(stats) + f"  {cost_desc}"
    
    async def process_single_prompt(prompt, index, progress, task_id):
        try:
            result = await async_call_openai_api(prompt, config)
            results[index] = result
            
            # å°†æ–°ç»“æœå†™å…¥ç¼“å­˜
            cache_key = prompt_to_key[prompt]
            cache.set(cache_key, result)
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            stats_desc = get_stats_description()
            current_model_type = config['current_model']['type']
            new_description = (
                f"promptså¤„ç†ä¸­ [æ¨¡å‹: {current_model_type}] [ç­‰å¾…å“åº”: {waiting_count}] [ä»»åŠ¡æ± : {len(tasks)-1}/{batch_size}] [æ€»tokens: {_total_tokens}]\n"
                f"{stats_desc}"
            )
            progress.update(
                task_id,
                description=new_description,
                advance=1
            )
            return result
        except Exception as e:
            gpt_logger.error(f"å¤„ç†æç¤ºè¯ {index} å¤±è´¥: {str(e)}")
            raise

    # åªå¤„ç†æœªå‘½ä¸­ç¼“å­˜çš„æç¤ºè¯
    if prompts_to_process:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(bar_width=None),
            TaskProgressColumn(),
            MofNCompleteColumn(),
            TimeElapsedColumn(),
            TimeRemainingColumn(),
            expand=True,
        ) as progress:
            current_model_type = config['current_model']['type']
            task_id = progress.add_task(
                description=f"promptså¤„ç†ä¸­ [æ¨¡å‹: {current_model_type}] [ç­‰å¾…å“åº”: {waiting_count}] [ä»»åŠ¡æ± : 0/{batch_size}] [æ€»tokens: {_total_tokens}]",
                total=len(prompts_to_process)
            )
            
            prompt_index = 0
            while prompt_index < len(prompts_to_process) and len(tasks) < batch_size:
                idx, prompt = prompts_to_process[prompt_index]
                task = asyncio.create_task(
                    process_single_prompt(prompt, idx, progress, task_id)
                )
                tasks.add(task)
                progress.update(
                    task_id,
                    description=f"promptså¤„ç†ä¸­ [æ¨¡å‹: {current_model_type}] [ç­‰å¾…å“åº”: {waiting_count}] [ä»»åŠ¡æ± : {len(tasks)}/{batch_size}] [æ€»tokens: {_total_tokens}]"
                )
                prompt_index += 1
            
            while tasks:
                done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                for task in done:
                    await task
                while prompt_index < len(prompts_to_process) and len(tasks) < batch_size:
                    idx, prompt = prompts_to_process[prompt_index]
                    task = asyncio.create_task(
                        process_single_prompt(prompt, idx, progress, task_id)
                    )
                    tasks.add(task)
                    progress.update(
                        task_id,
                        description=f"promptså¤„ç†ä¸­ [ç­‰å¾…å“åº”: {waiting_count}] [ä»»åŠ¡æ± : {len(tasks)}/{batch_size}] [æ€»tokens: {_total_tokens}]"
                    )
                    prompt_index += 1

    display_deployment_stats()
    # å¤„ç†å®Œæˆåï¼Œæ¢å¤é»˜è®¤æ¨¡å‹ç±»å‹
    if enforce_model_type != "default":
        config['current_model']['type'] = current_model_type_res
    return results

# ----------------------------------------------------------------------
# ä½¿ç”¨ Rich Table æ˜¾ç¤ºå„éƒ¨ç½²çš„ç»Ÿè®¡ä¿¡æ¯
# ----------------------------------------------------------------------
def display_deployment_stats():
    """
    ä½¿ç”¨ Rich Table æ˜¾ç¤ºå„ä¸ªéƒ¨ç½²çš„ç»Ÿè®¡ä¿¡æ¯ï¼š
    å±•ç¤ºéƒ¨ç½²ã€è¯·æ±‚æ¬¡æ•°ã€å“åº”æ—¶é—´ã€æ€» tokens æ•°ä»¥åŠå¹³å‡ tokens/ç§’ç­‰æ•°æ®ï¼Œ
    è¡¨æ ¼åº•éƒ¨æ˜¾ç¤ºæ€» token ä½¿ç”¨é‡åŠæˆæœ¬ä¿¡æ¯ã€‚
    """
    console = Console()
    table = Table(title="éƒ¨ç½²ç»Ÿè®¡ä¿¡æ¯", show_lines=True)

    table.add_column("éƒ¨ç½²", style="cyan", no_wrap=True)
    table.add_column("è¯·æ±‚æ¬¡æ•°", justify="center", style="green")
    table.add_column("å“åº”æ—¶é—´ (ç§’)", justify="center", style="magenta")
    table.add_column("å¹³å‡tokens/s", justify="center", style="red")

    for deployment, stat in _deployment_stats.items():
        table.add_row(
            deployment,
            str(stat.request_count),
            f"{stat.total_time:.2f}",
            f"{stat.average_tokens_per_second:.2f}"
        )

    input_cost = (_input_tokens / 1_000_000) * 2
    output_cost = (_output_tokens / 1_000_000) * 8
    table.caption = f"æ€»Tokens: {_total_tokens} | æˆæœ¬: è¾“å…¥ Â¥{input_cost:.2f}, è¾“å‡º Â¥{output_cost:.2f}"

    console.print(table)

async def process_single_prompt_special(prompt: str, config):
    """
    ç‰¹æ®Šå¤„ç†å•ä¸ªæç¤ºè¯ï¼Œä¸ä½¿ç”¨è¿›åº¦æ¡
    """
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = hashlib.md5(
        f"{prompt}_{config['current_model']['type']}".encode()
    ).hexdigest()
    
    # æ£€æŸ¥ç¼“å­˜
    cached_result = cache.get(cache_key)
    if cached_result is not None:
        logger.trace(f"å¤„ç†æç¤ºè¯: {prompt} å‘½ä¸­ç¼“å­˜ï¼Œç»“æœå·²è¿”å›ã€‚")
        return cached_result

    try:
        result = await async_call_openai_api(prompt, config)
        
        # å°†æ–°ç»“æœå†™å…¥ç¼“å­˜
        cache.set(cache_key, result)
        
        logger.trace(f"å¤„ç†æç¤ºè¯: {prompt} æˆåŠŸï¼Œç»“æœå·²ç¼“å­˜ã€‚")
        return result
    except Exception as e:
        logger.error(f"å¤„ç†æç¤ºè¯ {prompt} å¤±è´¥: {str(e)}")
        raise